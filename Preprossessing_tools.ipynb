{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:DataSciance]",
      "language": "python",
      "name": "conda-env-DataSciance-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Preprossessing_tools.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD7hXCLkd4VZ"
      },
      "source": [
        "# Preprosessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNtzOidfd4Vb"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG_lMVh2d4Vc"
      },
      "source": [
        "# random split \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MfwVwfNd4Ve"
      },
      "source": [
        "# one hot incoding\n",
        "\n",
        "\n",
        "one_hot_features = ['relationship', 'race', 'occupation', 'marital-status',\n",
        "                    'sex', 'workclass']\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "enc.fit(X_train[one_hot_features])\n",
        "X_train_enc_oh = pd.DataFrame(enc.transform(X_train[one_hot_features]).toarray(),\n",
        "                           columns=enc.get_feature_names())\n",
        "X_test_enc_oh = pd.DataFrame(enc.transform(X_test[one_hot_features]).toarray(),\n",
        "                          columns=enc.get_feature_names())\n",
        "\n",
        "X_train = pd.concat([X_train.drop(columns = one_hot_features), X_train_enc_oh], axis=1)\n",
        "del X_train_enc_oh\n",
        "\n",
        "X_test = pd.concat([X_test.drop(columns = one_hot_features), X_test_enc_oh], axis=1)\n",
        "del X_test_enc_oh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9_qHAYUd4Vf"
      },
      "source": [
        "# ordinal incoding\n",
        "\n",
        "ordinal_features = ['education']\n",
        "education_order = [' Preschool', ' 1st-4th', ' 5th-6th', ' 7th-8th', ' 9th',\n",
        "                   ' 10th', ' 11th', ' 12th',  ' HS-grad', ' Prof-school', \n",
        "                   ' Assoc-acdm', ' Assoc-voc', ' Some-college', ' Bachelors',\n",
        "                   ' Masters', ' Doctorate']\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "encoder = OrdinalEncoder(categories=[education_order])\n",
        "encoder.fit(X_train[ordinal_features].values.reshape(-1, 1))\n",
        "X_train_enc_ord = pd.DataFrame(encoder.transform(X_train[ordinal_features]),\n",
        "                           columns=ordinal_features)\n",
        "X_test_enc_ord = pd.DataFrame(encoder.transform(X_test[ordinal_features]),\n",
        "                          columns=ordinal_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCGbLc4Td4Vh"
      },
      "source": [
        "# Normalize feature\n",
        "\n",
        "mue = df[feature].mean()\n",
        "sigma = df[feature].std()\n",
        "df[feature] = (df[feature] - mue)/sigma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Lj-F1zd4Vi"
      },
      "source": [
        "# Dimensionality Reduction with PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def plot_PCA_curve(X_train):\n",
        "    pca = PCA()\n",
        "    pca.fit(X_train)\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('number of components')\n",
        "    plt.xlabel('cumulative explained variance')\n",
        "\n",
        "    plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
        "    plt.xlabel('k (component)')\n",
        "    plt.title('Percentage of variance explained by given component');\n",
        "\n",
        "\n",
        "def get_n_pca_components(n, X_train, X_test):\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(X_train)\n",
        "    X_train_t = pca.transform(X_train)\n",
        "    X_test_t = pca.transform(X_test)\n",
        "    return X_train_t, X_test_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UYfZAS1bx9H"
      },
      "source": [
        "import scipy\n",
        "from tqdm import tqdm\n",
        "\n",
        "def map_features(df, features_for_embedding, threshold=0.99):\n",
        "    lookup_dict = {}\n",
        "    data_mapped = df.copy(deep=True)\n",
        "\n",
        "    for col in tqdm(features_for_embedding):\n",
        "        \n",
        "        # Replace rare values with the string 'OOV'\n",
        "        normalized_vc = data_mapped[col].value_counts(normalize=True).cumsum()\n",
        "        vals_to_remove = list(normalized_vc[normalized_vc > threshold].index)\n",
        "        # print(f'Removing the following values from {col}: {vals_to_remove}')\n",
        "        data_mapped.loc[data_mapped[col].isin(vals_to_remove), col] = 'OOV' \n",
        "\n",
        "        # Create the mapping\n",
        "        col_mapping = {k: v for k, v in enumerate(data_mapped[col].unique(), start=1) if v != 'OOV'}\n",
        "        col_mapping[0] = 'OOV'\n",
        "        # rerank the keys to make sure that we have no missing key:\n",
        "        ranked_keys = scipy.stats.rankdata(list(col_mapping.keys())) - 1\n",
        "        reranked_col_mapping = dict(zip(ranked_keys, col_mapping.values()))\n",
        "        # add corresponding mapped columns\n",
        "        inverse_col_mapping = {v: k for k, v in reranked_col_mapping.items()}\n",
        "        data_mapped[f'{col}'] = data_mapped[col].map(inverse_col_mapping)\n",
        "\n",
        "        lookup_dict[col] = inverse_col_mapping\n",
        "\n",
        "    # display(lookup_dict)\n",
        "    # display(data_mapped)\n",
        "    return data_mapped, lookup_dict\n",
        "\n",
        "def map_test(df_test: pd.DataFrame, lookup_dict: dict):\n",
        "    data_mapped = pd.DataFrame()\n",
        "    for key in tqdm(lookup_dict.keys()):\n",
        "        data_mapped[key] = df_test_t[key].apply(lambda x: lookup_dict[key][x]\n",
        "                                            if x in lookup_dict[key].keys()\n",
        "                                            else 0)\n",
        "    return data_mapped\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}