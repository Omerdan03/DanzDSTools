{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Feature engineering.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDOQA5pFa_-C"
      },
      "source": [
        "# categorical feature selection by percentile\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "k=15\n",
        "original_features = set(X_train.columns)\n",
        "fs = SelectKBest(score_func=chi2, k=k)\n",
        "fs.fit(X_train, y_train)\n",
        "X_train_fs = fs.transform(X_train)\n",
        "X_test_fs = fs.transform(X_test)\n",
        "\n",
        "if k != 'all':\n",
        "    dropped_features = list(X_train.iloc[:,fs.scores_.argsort()[k:]].columns)\n",
        "else:\n",
        "    dropped_features = []\n",
        "print(f'The features that were dropped with the Chi-square method were: {dropped_features}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfG2k2kXa_-D"
      },
      "source": [
        "# categorical feature selection by percentile\n",
        "\n",
        "from sklearn.feature_selection import SelectPercentile, chi2\n",
        "\n",
        "percentile = 10\n",
        "original_features = set(X_train.columns)\n",
        "fs = SelectPercentile(score_func=chi2, percentile=percentile)\n",
        "fs.fit(X_train, y_train)\n",
        "X_train_fs = fs.transform(X_train)\n",
        "X_test_fs = fs.transform(X_test)\n",
        "\n",
        "dropped_features = list(X_train.iloc[:, fs.scores_ <= np.percentile(fs.scores_, 100-percentile)].columns)\n",
        "kept_features = list(X_train.iloc[:, fs.scores_ > np.percentile(fs.scores_, 100-percentile)].columns)\n",
        "print(f'The features that were dropped are: {dropped_features}')\n",
        "print(f'The features that were kept are: {kept_features}')\n",
        "best_categorical = kept_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ahGU70na_-E"
      },
      "source": [
        "#  numerical feature selection by low variance\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "threshold = (20)\n",
        "\n",
        "original_features = numerical\n",
        "fs = VarianceThreshold(threshold=threshold)\n",
        "fs.fit(X_train[numerical], y_train)\n",
        "X_train_fs = fs.transform(X_train[numerical])\n",
        "X_test_fs = fs.transform(X_test[numerical])\n",
        "pick = fs.get_support()\n",
        "kept_features = list(np.array(original_features)[pick])\n",
        "dropped_features = list(set(original_features) - set(kept_features))\n",
        "print(f'The features that were dropped are: {dropped_features}')\n",
        "print(f'The features that were kept are: {kept_features}')\n",
        "best_numerical = kept_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcxK7Qala_-G"
      },
      "source": [
        "# feature selection using DecisionTreeClassifier\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=10)\n",
        "rfe.fit(X_train, y_train)\n",
        "X_train_fs = rfe.transform(X_train)\n",
        "X_test_fs = rfe.transform(X_test)\n",
        "\n",
        "dropped_features = list(X_train.loc[:,~rfe.support_].columns)\n",
        "kept_features = list(X_train.loc[:,rfe.support_].columns)\n",
        "\n",
        "print(f'The features that were dropped are: {dropped_features}')\n",
        "print(f'The features that were kept are: {kept_features}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heks6DtYa_-Q"
      },
      "source": [
        "# Dimensionlity Reduction with PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_PCA_curve(X_train):\n",
        "    pca = PCA()\n",
        "    pca.fit(X_train)\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('number of components')\n",
        "    plt.xlabel('cumulative explained variance')\n",
        "\n",
        "def explain_pca(num_pca, X):\n",
        "    pca = PCA()\n",
        "    pca.fit(X)\n",
        "    columns = X.columns\n",
        "    for i in range(num_pca):\n",
        "        importantness = pca.components_[i]\n",
        "        sns.barplot(x=importantness, y=columns)\n",
        "        plt.title(f'importantness of component {i+1}')\n",
        "\n",
        "def get_n_pca_components(n, X_train, X_test):\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(X_train)\n",
        "    X_train_t = pca.transform(X_train)\n",
        "    X_test_t = pca.transform(X_test)\n",
        "    return X_train_t, X_test_t"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}