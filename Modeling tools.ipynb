{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, plot_roc_curve\n",
    "\n",
    "def print_roc(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    plot_roc_curve(clf, X_test, y_test)\n",
    "    plt.plot([(0,0),(1,1)], '--y')\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(model, X_test, y_test, normalize=False, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = 'Normalized confusion matrix'\n",
    "    else:\n",
    "        title = 'Confusion matrix, without normalization'\n",
    "    classes = np.arange(len(model.classes_))\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    np.set_printoptions(precision=2)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def data_split(data, features, target, split_type='random', test_size=0.2):\n",
    "    \"\"\"\n",
    "    This function split a df to X_train, X_test, y_train and y_train from a given database, \n",
    "    features list and target feature\n",
    "    \"\"\"\n",
    "    split_types_avilable = ['random']\n",
    "    if split_type not in split_types_avilable:\n",
    "        raise ValueError(f\"split_type is not within the scop of the function can be on of{split_types_avilable}\")\n",
    "    X = data[feature]\n",
    "    y = data[target]\n",
    "    if split_type = 'random':\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random=42)\n",
    "    \n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical feature selection by percentile\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "k=15\n",
    "original_features = set(X_train.columns)\n",
    "fs = SelectKBest(score_func=chi2, k=k)\n",
    "fs.fit(X_train, y_train)\n",
    "X_train_fs = fs.transform(X_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "\n",
    "if k != 'all':\n",
    "    dropped_features = list(X_train.iloc[:,fs.scores_.argsort()[k:]].columns)\n",
    "else:\n",
    "    dropped_features = []\n",
    "print(f'The features that were dropped with the Chi-square method were: {dropped_features}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical feature selection by percentile\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "percentile = 10\n",
    "original_features = set(X_train.columns)\n",
    "fs = SelectPercentile(score_func=chi2, percentile=percentile)\n",
    "fs.fit(X_train, y_train)\n",
    "X_train_fs = fs.transform(X_train)\n",
    "X_test_fs = fs.transform(X_test)\n",
    "\n",
    "dropped_features = list(X_train.iloc[:, fs.scores_ <= np.percentile(fs.scores_, 100-percentile)].columns)\n",
    "kept_features = list(X_train.iloc[:, fs.scores_ > np.percentile(fs.scores_, 100-percentile)].columns)\n",
    "print(f'The features that were dropped are: {dropped_features}')\n",
    "print(f'The features that were kept are: {kept_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection using DecisionTreeClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=10)\n",
    "rfe.fit(X_train, y_train)\n",
    "X_train_fs = rfe.transform(X_train)\n",
    "X_test_fs = rfe.transform(X_test)\n",
    "\n",
    "dropped_features = list(X_train.loc[:,~rfe.support_].columns)\n",
    "kept_features = list(X_train.loc[:,rfe.support_].columns)\n",
    "\n",
    "print(f'The features that were dropped are: {dropped_features}')\n",
    "print(f'The features that were kept are: {kept_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting RPC curce and metrics\n",
    "\n",
    "from sklearn.metrics import classification_report, plot_roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "print(classification_report(y_test, y_pred))\n",
    "plot_roc_curve(clf, X_test, y_test)\n",
    "plt.plot([(0,0),(1,1)], '--y')\n",
    "plt.title('ROC curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting and printing regression modles\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def train_print_metrics(reg, X_train, y_train, X_test, y_test):\n",
    "    print(type(reg).__name__)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    print(f'mean squared error {mean_squared_error(y_test, y_pred)}')\n",
    "    print(f'R squard score {r2_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting precision and recall based on threshold\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_probs = clf.predict_proba(X_test)[:,1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "\n",
    "def linear_inter(x1, y1, x2, y2, x):\n",
    "    m = (y2-y1)/(x2-x1)\n",
    "    y = y1 + (x2-x) * m\n",
    "    return y\n",
    "\n",
    "\n",
    "recall_ = 0.7\n",
    "for index, rec in enumerate(recall):\n",
    "    if rec < recall_:\n",
    "        break\n",
    "    \n",
    "precision_ = linear_inter(recall[index-1], precision[index-1], recall[index],\n",
    "             precision[index], recall_)\n",
    "threshold_ = linear_inter(recall[index-1], thresholds[index-1], recall[index],\n",
    "             thresholds[index], recall_)\n",
    "print(f'The precision on (recall={recall_}) is {precision_}')\n",
    "print(f'The threshold on (recall={recall_}) is {threshold_}')\n",
    "\n",
    "plt.plot(thresholds, precision[:-1], label='precision')\n",
    "plt.plot(thresholds, recall[:-1], label='recall')\n",
    "plt.xlabel('threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation k-Fold\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "reg = LogisticRegression()\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train2, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train2, val = y[train_index], y[val_index]\n",
    "    \n",
    "final_score = cross_val_score(reg, X_train, y_train, cv= kf, scoring=\"accuracy\")\n",
    "print(f'Scores for each fold: {final_score}')\n",
    "print('Final Model Score: %.2f' %(final_score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search CV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "cls = LogisticRegression()\n",
    "\n",
    "params_dict = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "               'tol': [10**-5, 10**-4, 10**-3],\n",
    "               'C': [1.5, 1, 0.7],\n",
    "               'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "print(f'{type(reg).__name__} Tuning hyper-parameters with grid')\n",
    "\n",
    "ss_cv = ShuffleSplit(n_splits=5)\n",
    "\n",
    "clf_forest = GridSearchCV(cls, params_dict, cv = ss_cv, verbose=10, n_jobs=-1)\n",
    "\n",
    "clf_forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on validation set:\")\n",
    "print(clf_forest.best_params_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search CV\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "reg = LogisticRegression()\n",
    "\n",
    "params_dict = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "               'tol': [10**-6, 10**-5, 10**-4, 10**-3],\n",
    "               'C': [2, 1.5, 1, 0.7],\n",
    "               'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "\n",
    "print(f'{type(reg).__name__} Tuning hyper-parameters with grid')\n",
    "\n",
    "ss_cv = ShuffleSplit(n_splits=5)\n",
    "\n",
    "clf_forest = RandomizedSearchCV(reg, params_dict, random_state=42, \n",
    "                                cv = ss_cv, verbose=10, n_iter=200, n_jobs=-1)\n",
    "\n",
    "clf_forest.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set found on validation set:\")\n",
    "print(clf_forest.best_params_, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionlity Reduction with PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_PCA_curve(X_train):\n",
    "    pca = PCA()\n",
    "    pca.fit(X_train)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.xlabel('cumulative explained variance')\n",
    "\n",
    "\n",
    "def get_n_pca_components(n, X_train, X_test):\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train)\n",
    "    X_train_t = pca.transform(X_train)\n",
    "    X_test_t = pca.transform(X_test)\n",
    "    return X_train_t, X_test_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
